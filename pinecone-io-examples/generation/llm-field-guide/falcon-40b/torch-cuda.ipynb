{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e611b5a-d195-4ef4-a50a-9a978b513afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0874369f-885f-49c1-9d39-4666d4d73f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "422bf548-1547-4994-bb8d-607700be68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c681110d-ec48-45ba-b7b5-cc4bc72751cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "098ee5fa-61c5-4e33-8265-86d00ceeb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16952a63-c061-41b4-888a-3424d63086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "    #load_in_4bit=True,\n",
    "    load_in_4bit_fp32_cpu_offload=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507c8f5d-c5f4-405e-920a-749afe186661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'tiiuae/falcon-40b-instruct'\n",
    "# model_name = 'tiiuae/falcon-7b-instruct'\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd4dd0f0-b2c0-4457-93d6-3b79ec5b1574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69aa5e3c67ac4c5ea5aca6be017df00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gourab_deb/Projects/VsCode/langchain-experiments/pinecone-io-examples/generation/env-torch-cuda/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0941da9134d44430a3750ea8c6ef8f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95d7491945b459b967a88c943f82988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5486cedc0bd04db38eea1add71d6e769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766071cf3837453e8f48694581e7c09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbb9aee8568448e97fd377317077663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119cecaf6fc64a5b88ba1c57fb195b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n",
      "CPU times: user 1min 9s, sys: 1min, total: 2min 10s\n",
      "Wall time: 20min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=True,\n",
    "    offload_folder=\"/mnt/d/AAA/VsCode/offload/meta-Llama-2-7b-chat-hf\" # Disk path to load the model when GPU, CPU runs out of memo\n",
    "    #device_map=device_map \n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79e8ad28-6559-4106-ab95-908ffa1492cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e451f504f2f4a9bbc8438b419b1609d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gourab_deb/Projects/VsCode/langchain-experiments/pinecone-io-examples/generation/env-torch-cuda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6b113275ab455c8e2ac4d9d4213fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d135b98bd3d482c90f54b01f154c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2187a4e053e44519d9f4bdeab74d0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e293ad45-55ad-46ec-94bd-0fd88c6a3737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 29901], [23869, 29901]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# we create a list of stopping criteria\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [\n",
    "        ['Human', ':'], ['AI', ':']\n",
    "    ]\n",
    "]\n",
    "\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eeb7548c-7d4c-4398-aade-5e4840ff75cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    0, 29901], device='cuda:0'),\n",
       " tensor([23869, 29901], device='cuda:0')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to convert these into `LongTensor` objects:\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd7c16e2-c83d-43f4-a9e7-f588159a062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1b472f0-5d40-41ed-a403-38900596b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to initialize the HF pipeline. There are a few additional parameters that we must define here. Comments explaining these have been included in the code.\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a399c-592e-4592-9700-c95eae1abd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Confirm this is working\n",
    "res = generate_text(\"Explain to me the difference between nuclear fission and fusion.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142e79eb-0fde-489b-8a0c-4427f69e99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92cd2c1-ee96-4aa8-83ab-46f93b1df543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for an instruction with no input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\"],\n",
    "    template=\"{instruction}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a8161b-af4c-4213-8c22-2279c3f93e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 262 µs, sys: 98 µs, total: 360 µs\n",
      "Wall time: 387 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a48565a-45d9-4a89-b22b-f77138b88d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear fission is a nuclear reaction in which the nucleus of an atom splits into two smaller nuclei, releasing a large amount of energy in the form of radiation and kinetic energy of the fragments. Fusion, on the other hand, is a nuclear reaction in which two lighter atomic nuclei combine to form a heavier atomic nucleus, releasing a large amount of energy in the form of radiation and kinetic energy of the resulting nucleus. In both reactions, the binding energy of the nucleus is released, allowing for the release of a significant amount of energy.\n",
      "CPU times: user 6min 32s, sys: 2min 17s, total: 8min 50s\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(llm_chain.predict(\n",
    "    instruction=\"Explain to me the difference between nuclear fission and fusion.\"\n",
    ").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca473e-1992-4b03-9eff-ef7b4613b1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-torch-cuda",
   "language": "python",
   "name": "env-torch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
